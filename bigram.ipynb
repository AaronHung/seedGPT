{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20979cea",
   "metadata": {},
   "source": [
    "---\n",
    "### Try to open the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c8ce59c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DOROTHY AND THE WIZARD IN OZ\n",
      "\n",
      "  BY\n",
      "\n",
      "  L. FRANK BAUM\n",
      "\n",
      "  AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
      "\n",
      "  ILLUSTRATED BY JOHN R. NEILL\n",
      "\n",
      "  BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW\n"
     ]
    }
   ],
   "source": [
    "with open('data/wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "95583f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DOROTHY AND THE WIZARD IN OZ\n",
      "\n",
      "  BY\n",
      "\n",
      "  L. FRANK BAUM\n",
      "\n",
      "  AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
      "\n",
      "  ILLUSTRATED BY JOHN R. NEILL\n",
      "\n",
      "  BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt') as f:\n",
    "    text = f.read()\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b019714",
   "metadata": {},
   "source": [
    "--- \n",
    "### To process the chars / diff. way to format output length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2559e8e",
   "metadata": {},
   "source": [
    "If you try to sort all the characters, some might duplicate,\n",
    "therefore use a `set` to remove the duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1b19e1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17306948",
   "metadata": {},
   "source": [
    "#### or use pprint to look at what charcters are there in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8c458780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n',\n",
      " ' ',\n",
      " '!',\n",
      " '\"',\n",
      " '&',\n",
      " \"'\",\n",
      " '(',\n",
      " ')',\n",
      " '*',\n",
      " ',',\n",
      " '-',\n",
      " '.',\n",
      " '0',\n",
      " '1',\n",
      " '2',\n",
      " '3',\n",
      " '4',\n",
      " '5',\n",
      " '6',\n",
      " '7',\n",
      " '8',\n",
      " '9',\n",
      " ':',\n",
      " ';',\n",
      " '?',\n",
      " 'A',\n",
      " 'B',\n",
      " 'C',\n",
      " 'D',\n",
      " 'E',\n",
      " 'F',\n",
      " 'G',\n",
      " 'H',\n",
      " 'I',\n",
      " 'J',\n",
      " 'K',\n",
      " 'L',\n",
      " 'M',\n",
      " 'N',\n",
      " 'O',\n",
      " 'P',\n",
      " 'Q',\n",
      " 'R',\n",
      " 'S',\n",
      " 'T',\n",
      " 'U',\n",
      " 'V',\n",
      " 'W',\n",
      " 'X',\n",
      " 'Y',\n",
      " 'Z',\n",
      " '[',\n",
      " ']',\n",
      " '_',\n",
      " 'a',\n",
      " 'b',\n",
      " 'c',\n",
      " 'd',\n",
      " 'e',\n",
      " 'f',\n",
      " 'g',\n",
      " 'h',\n",
      " 'i',\n",
      " 'j',\n",
      " 'k',\n",
      " 'l',\n",
      " 'm',\n",
      " 'n',\n",
      " 'o',\n",
      " 'p',\n",
      " 'q',\n",
      " 'r',\n",
      " 's',\n",
      " 't',\n",
      " 'u',\n",
      " 'v',\n",
      " 'w',\n",
      " 'x',\n",
      " 'y',\n",
      " 'z',\n",
      " '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(chars, width=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "47b0ab75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"&'()*,-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXY\n",
      "Z[]_abcdefghijklmnopqrstuvwxyzï»¿\n"
     ]
    }
   ],
   "source": [
    "# Wrap characters into a string with line breaks every 50 chars\n",
    "chars = sorted(set(text))\n",
    "joined = ''.join(chars)\n",
    "\n",
    "# Break into multiple lines for readability\n",
    "for i in range(0, len(joined), 50):\n",
    "    print(joined[i:i+50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "89b8eb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(sorted(set(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f99b63f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "ip = get_ipython()\n",
    "ip.display_formatter.formatters['text/plain'].max_seq_length = 30  # Wrap after 30 items\n",
    "\n",
    "print(sorted(set(text)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0077f79",
   "metadata": {},
   "source": [
    "#### Write my own print formatting for jupyter notebook cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "817d6fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.']\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';']\n",
      "['?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W']\n",
      "['X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f']\n",
      "['g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r']\n",
      "['s', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "\n",
    "def aprint(chars, chars_per_line=12):\n",
    "    for i in range(0, len(chars), chars_per_line):\n",
    "        print(chars[i:i+chars_per_line])\n",
    "        \n",
    "aprint(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa00b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f3e62d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '&': 4, \"'\": 5, '(': 6, ')': 7, '*': 8, ',': 9, '-': 10, '.': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '?': 24, 'A': 25, 'B': 26, 'C': 27, 'D': 28, 'E': 29, 'F': 30, 'G': 31, 'H': 32, 'I': 33, 'J': 34, 'K': 35, 'L': 36, 'M': 37, 'N': 38, 'O': 39, 'P': 40, 'Q': 41, 'R': 42, 'S': 43, 'T': 44, 'U': 45, 'V': 46, 'W': 47, 'X': 48, 'Y': 49, 'Z': 50, '[': 51, ']': 52, '_': 53, 'a': 54, 'b': 55, 'c': 56, 'd': 57, 'e': 58, 'f': 59, 'g': 60, 'h': 61, 'i': 62, 'j': 63, 'k': 64, 'l': 65, 'm': 66, 'n': 67, 'o': 68, 'p': 69, 'q': 70, 'r': 71, 's': 72, 't': 73, 'u': 74, 'v': 75, 'w': 76, 'x': 77, 'y': 78, 'z': 79, '\\ufeff': 80}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_int = {c: i for i, c in enumerate(chars)}\n",
    "print(string_to_int)\n",
    "\n",
    "string_to_int['&']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "75b32525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '&', 5: \"'\", 6: '(', 7: ')', 8: '*', 9: ',', 10: '-', 11: '.', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '?', 25: 'A', 26: 'B', 27: 'C', 28: 'D', 29: 'E', 30: 'F', 31: 'G', 32: 'H', 33: 'I', 34: 'J', 35: 'K', 36: 'L', 37: 'M', 38: 'N', 39: 'O', 40: 'P', 41: 'Q', 42: 'R', 43: 'S', 44: 'T', 45: 'U', 46: 'V', 47: 'W', 48: 'X', 49: 'Y', 50: 'Z', 51: '[', 52: ']', 53: '_', 54: 'a', 55: 'b', 56: 'c', 57: 'd', 58: 'e', 59: 'f', 60: 'g', 61: 'h', 62: 'i', 63: 'j', 64: 'k', 65: 'l', 66: 'm', 67: 'n', 68: 'o', 69: 'p', 70: 'q', 71: 'r', 72: 's', 73: 't', 74: 'u', 75: 'v', 76: 'w', 77: 'x', 78: 'y', 79: 'z', 80: '\\ufeff'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'&'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_string = {i: c for c, i in string_to_int.items()}\n",
    "print(int_to_string)\n",
    "\n",
    "int_to_string[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c49f2a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 58, 65, 65, 68]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode = lambda s: [ string_to_int[c] for c in s ]\n",
    "\n",
    "encode('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bd301348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode = lambda l: ''.join([ int_to_string[i] for i in l ])\n",
    "\n",
    "decode(encode('hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d329fe3",
   "metadata": {},
   "source": [
    "#### put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "871a7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = {ch: i for i, ch in enumerate(set(text))}\n",
    "int_to_string = {i: ch for ch, i in string_to_int.items()}\n",
    "encode = lambda s: [string_to_int[ch] for ch in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9c05c4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 6, 77, 77, 62]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a8574c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 6, 77, 77, 62] hello\n"
     ]
    }
   ],
   "source": [
    "encoded_hello = encode('hello')\n",
    "decoded_hello = decode(encoded_hello)\n",
    "\n",
    "print(encoded_hello, decoded_hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ee8f6",
   "metadata": {},
   "source": [
    "### Some background information of these tokenizers\n",
    "\n",
    "- right now, we're using the character level tokenizer, which converting each character into integer\n",
    "- so we have `a small vocabulary` (characters as vocabulary), and a very large amount of `tokens` to convert. We have `40,000` individual characters -- we have a small vocabulary to work with\n",
    "- if we work with a word-level tokenizer, every single words in English, that means we have a TON, like every single word in English language, and worse if you have multiple languages.\n",
    "- It's millions or trillions if you're doing something wiered. In this case, the tokenizer vocabulary are huge, but way smaller set to work with. (Very large vocabulary, very small to encode/decode)\n",
    "- if you are a `sub-word` tokenizer - that means you are some way in between."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26009a68",
   "metadata": {},
   "source": [
    "---\n",
    "### Change every data into `pytorch` or `torch` as `tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0a1d2c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is: mps\n",
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n",
      "vocabulary_size = 81\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# For MAC, the GPU is MPS\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'Device is: {device}')\n",
    "\n",
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocabulary_size = len(chars)\n",
    "print(f'vocabulary_size = {vocabulary_size}')\n",
    "\n",
    "\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [char_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_char[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8f808047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([232309]) torch.int64\n",
      "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
      "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
      "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,\n",
      "         0,  0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1,\n",
      "        36, 25, 38, 28,  1, 39, 30,  1, 39, 50])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c646e",
   "metadata": {},
   "source": [
    "---\n",
    "### Validation and Training Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ef271",
   "metadata": {},
   "source": [
    "> - Why we just don't use the entire document/text and only train on that?\n",
    "> - The reason we split into train and validation sets,..(later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "209fa1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "63fe7e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([185847]) torch.Size([46462])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.size(),  val_data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1836ae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ab4b6853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([80]) the target is: 1\n",
      "when input is tensor([80,  1]) the target is: 1\n",
      "when input is tensor([80,  1,  1]) the target is: 28\n",
      "when input is tensor([80,  1,  1, 28]) the target is: 39\n",
      "when input is tensor([80,  1,  1, 28, 39]) the target is: 42\n",
      "when input is tensor([80,  1,  1, 28, 39, 42]) the target is: 39\n",
      "when input is tensor([80,  1,  1, 28, 39, 42, 39]) the target is: 44\n",
      "when input is tensor([80,  1,  1, 28, 39, 42, 39, 44]) the target is: 32\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "# this is like a sliding window, where we take 8 chars at a time\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context} the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b0d92c",
   "metadata": {},
   "source": [
    "---\n",
    "###  Some experiments about torch's matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9706b1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 82,  31,  46,  43,  -8, -15])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randint = torch.randint(-100, 100, (6,))\n",
    "randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce75ef2",
   "metadata": {},
   "source": [
    "Create a 2 x 3 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a0b54159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 1.2000],\n",
       "        [2.2000, 3.1000],\n",
       "        [4.9000, 5.2000]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5f06ae75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = torch.zeros(2, 3)\n",
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0fa1f231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones(3, 4)\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "224ffc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.empty(2, 3)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ae268795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arange = torch.arange(5)\n",
    "arange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "19b42f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linspace = torch.linspace(3, 10, steps=5)\n",
    "linspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a55eb7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-10, 1.0000e-05, 1.0000e+00, 1.0000e+05, 1.0000e+10])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logspace = torch.logspace(start=-10, end=10, steps=5)\n",
    "logspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a672f27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eye = torch.eye(5)\n",
    "eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0e5e6f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.empty((2, 3), dtype=torch.int64)\n",
    "empty_like = torch.empty_like(a)\n",
    "empty_like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b49897",
   "metadata": {},
   "source": [
    "---\n",
    "### Experiments with `time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b5feee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "batch_size = 4  # how many blocks we're doing in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "12b7dad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'Device is: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "645c99a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1745288254.388303"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0b75a1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse_time = 0.00022507\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "zeros = torch.zeros(1, 1)\n",
    "end_time = time.time()\n",
    "\n",
    "elapse_time = end_time - start_time\n",
    "print(f\"elapse_time = {elapse_time:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97210a1f",
   "metadata": {},
   "source": [
    "#### ð Comparing `time` for GPU vs CPU (with torch vs numpy matrix multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c8961fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_time = 4.59506989\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "torch_rand1 = torch.rand(10000, 10000).to(device)\n",
    "torch_rand1\n",
    "end_time = time.time()\n",
    "print(f\"elapsed_time = {end_time - start_time:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "02538977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse_time = 0.00842094\n"
     ]
    }
   ],
   "source": [
    "torch_rand1 = torch.rand(10000, 10000).to(device)\n",
    "torch_rand2 = torch.rand(10000, 10000).to(device)\n",
    "np_rand1 = torch.rand(10000, 10000)\n",
    "np_rand2 = torch.rand(10000, 10000)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"elapse_time = {elapsed_time:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a3e255cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_time: 0.26568413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/01/2j7mx2qs27j6cjvfw8cs8ps80000gn/T/ipykernel_60329/1483514504.py:3: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  rand = np.multiply(np_rand1, np_rand2)\n"
     ]
    }
   ],
   "source": [
    "star_time = time.time()\n",
    "\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"elapsed_time: {elapsed_time:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "bdd0f81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse_time = 0.01720190\n",
      "elapsed_time = 0.08940887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/01/2j7mx2qs27j6cjvfw8cs8ps80000gn/T/ipykernel_60329/2434095031.py:17: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  rand = np.multiply(np_rand1, np_rand2)\n"
     ]
    }
   ],
   "source": [
    "torch_rand1 = torch.rand(10000, 10000).to(device)\n",
    "torch_rand2 = torch.rand(10000, 10000).to(device)\n",
    "np_rand1 = torch.rand(10000, 10000)  # these causes complaints about deprecation \n",
    "np_rand2 = torch.rand(10000, 10000)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"elapse_time = {elapsed_time:.8f}\")\n",
    "\n",
    "star_time = time.time()\n",
    "\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"elapsed_time = {elapsed_time:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f90d813d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse_time = 0.01474571\n",
      "elapsed_time = 0.11285996\n"
     ]
    }
   ],
   "source": [
    "torch_rand1 = torch.rand(10000, 10000).to(device)\n",
    "torch_rand2 = torch.rand(10000, 10000).to(device)\n",
    "np_rand1 = torch.rand(10000, 10000).numpy()  # no more complains\n",
    "np_rand2 = torch.rand(10000, 10000).numpy()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"elapse_time = {elapsed_time:.8f}\")\n",
    "\n",
    "star_time = time.time()\n",
    "\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"elapsed_time = {elapsed_time:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f63043",
   "metadata": {},
   "source": [
    "##### ð³ lets create more dimensions for tensor and numpy to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f687ba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse_time = 0.01447082\n",
      "elapsed_time = 0.07774997\n"
     ]
    }
   ],
   "source": [
    "torch_rand1 = torch.rand(100, 100, 100, 100).to(device)\n",
    "torch_rand2 = torch.rand(100, 100, 100, 100).to(device)\n",
    "np_rand1 = torch.rand(100, 100, 100, 100).numpy()\n",
    "np_rand2 = torch.rand(100, 100, 100, 100).numpy()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"elapse_time = {elapsed_time:.8f}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"elapsed_time = {elapsed_time:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420053af",
   "metadata": {},
   "source": [
    "## ð What went `WRONG` since GPU seems slower than CPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b2568",
   "metadata": {},
   "source": [
    " ### Something **isn't quite right** in our codes above and results.\n",
    "---\n",
    "\n",
    "### ð 1. **Biggest Issue: `@` operator on 4D tensors is undefined!**\n",
    "\n",
    "```python\n",
    "rand = (torch_rand1 @ torch_rand2)\n",
    "```\n",
    "\n",
    "This line is **invalid math**: the `@` (matrix multiplication) operator is **defined only for 2D (matrices)** or **batched 3D tensors**.\n",
    "\n",
    "Our tensors are shape **`(100, 100, 100, 100)`** â so `@` might **not even be doing what you think**. It could raise an error in newer PyTorch or behave unexpectedly (and slowly).\n",
    "\n",
    "---\n",
    "\n",
    "#### â Fix it: Use `torch.matmul` explicitly and reshape\n",
    "\n",
    "If we want to test **matrix multiplication** speed, we'll need **2D or batch matrix multiply**. For example:\n",
    "\n",
    "```python\n",
    "# Reduce shape to valid matmul: (10000, 10000)\n",
    "A = torch.rand(10000, 10000, device=device)\n",
    "B = torch.rand(10000, 10000, device=device)\n",
    "\n",
    "start = time.time()\n",
    "C = A @ B\n",
    "torch.cuda.synchronize() if device == 'cuda' else None\n",
    "end = time.time()\n",
    "print(f\"GPU matmul time: {end - start:.6f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ð¥ 2. **Missing GPU Sync**\n",
    "\n",
    "In our original GPU code:\n",
    "\n",
    "```python\n",
    "start_time = time.time()\n",
    "rand = torch_rand1 @ torch_rand2\n",
    "end_time = time.time()\n",
    "```\n",
    "\n",
    "GPU operations are **asynchronous** by default, so `end_time` might be recorded before the GPU actually finishes computation.\n",
    "\n",
    "#### â Fix it: Add `torch.cuda.synchronize()`\n",
    "\n",
    "```python\n",
    "start_time = time.time()\n",
    "rand = torch_rand1 @ torch_rand2\n",
    "torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "end_time = time.time()\n",
    "```\n",
    "\n",
    "For M1/M2 macs (Apple Silicon):\n",
    "```python\n",
    "torch.mps.synchronize()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### â ï¸ 3. **NumPy tensors are NOT NumPy!**\n",
    "\n",
    "This is dangerous:\n",
    "\n",
    "```python\n",
    "np_rand1 = torch.rand(...).numpy()\n",
    "```\n",
    "\n",
    "We're converting **PyTorch tensors to NumPy** using `.numpy()`, but **only works if on CPU**.\n",
    "\n",
    "If they're still on GPU (or MPS), this will error or give wrong result.\n",
    "\n",
    "#### â Fix:\n",
    "\n",
    "```python\n",
    "np_rand1 = torch.rand(100, 100).cpu().numpy()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### â Final Cleaned Version: Proper GPU vs CPU benchmark\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Use 2D matrix for fair test\n",
    "torch_rand1 = torch.rand(10000, 10000).to(device)\n",
    "torch_rand2 = torch.rand(10000, 10000).to(device)\n",
    "\n",
    "# GPU test\n",
    "start = time.time()\n",
    "rand = torch_rand1 @ torch_rand2\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "elif device == \"mps\":\n",
    "    torch.mps.synchronize()\n",
    "end = time.time()\n",
    "print(f\"GPU matmul time = {end - start:.6f} sec\")\n",
    "\n",
    "# CPU NumPy test (element-wise mult)\n",
    "np_rand1 = torch.rand(10000, 10000).cpu().numpy()\n",
    "np_rand2 = torch.rand(10000, 10000).cpu().numpy()\n",
    "\n",
    "start = time.time()\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "end = time.time()\n",
    "print(f\"CPU np.multiply time = {end - start:.6f} sec\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ð§  Summary\n",
    "\n",
    "| Problem                         | Fix                                                       |\n",
    "|----------------------------------|------------------------------------------------------------|\n",
    "| â Invalid `@` for 4D tensors     | â Use 2D or batched 3D with `@` or `matmul`                |\n",
    "| â No GPU sync                   | â Add `torch.cuda.synchronize()` before timing ends       |\n",
    "| â NumPy from GPU tensor         | â Move to CPU with `.cpu().numpy()`                       |\n",
    "\n",
    "---\n",
    "\n",
    "We may also like to also compare `torch.mul()` vs `np.multiply()` or include `cupy` as a GPU-accelerated NumPy alternative. We'll explore later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4f4f1dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "GPU matmul time = 0.820652 sec\n",
      "CPU np.multiply time = 0.082828 sec\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Use 2D matrix for fair test\n",
    "torch_rand1 = torch.rand(10000, 10000).to(device)\n",
    "torch_rand2 = torch.rand(10000, 10000).to(device)\n",
    "\n",
    "# GPU test\n",
    "start = time.time()\n",
    "rand = torch_rand1 @ torch_rand2\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "elif device == \"mps\":\n",
    "    torch.mps.synchronize()\n",
    "end = time.time()\n",
    "print(f\"GPU matmul time = {end - start:.6f} sec\")\n",
    "\n",
    "# CPU NumPy test (element-wise mult)\n",
    "np_rand1 = torch.rand(10000, 10000).cpu().numpy()\n",
    "np_rand2 = torch.rand(10000, 10000).cpu().numpy()\n",
    "\n",
    "start = time.time()\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "end = time.time()\n",
    "print(f\"CPU np.multiply time = {end - start:.6f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1181b89",
   "metadata": {},
   "source": [
    "### ð still seems wrong, what happened (`unfair benchmark`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6655eec8",
   "metadata": {},
   "source": [
    "## ð§¨ The core issue:\n",
    "we're comparing **two *fundamentally different operations***:\n",
    "\n",
    "| Operation                        | What it does                                  | Complexity |\n",
    "|----------------------------------|-----------------------------------------------|------------|\n",
    "| `@` or `matmul` (on GPU)         | **Matrix multiplication**: `A @ B`            | **O(nÂ³)**   |\n",
    "| `np.multiply` (on CPU)           | **Element-wise multiplication**: `A * B`      | **O(n)**    |\n",
    "\n",
    "\n",
    "> â we're comparing **heavy matrix math** vs **lightweight point-wise math** â theyâre not meant to take equal time.\n",
    "\n",
    "---\n",
    "\n",
    "## â Letâs make it a *fair benchmark*\n",
    "\n",
    "Weâll compare **the same operation** across **CPU vs GPU**:\n",
    "\n",
    "### ð§ª A. Matrix Multiplication: GPU vs CPU\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import time\n",
    "\n",
    "size = 4000  # careful! 10000 x 10000 may crash CPU RAM\n",
    "\n",
    "A_cpu = torch.rand(size, size)\n",
    "B_cpu = torch.rand(size, size)\n",
    "\n",
    "# CPU matmul\n",
    "start = time.time()\n",
    "result = A_cpu @ B_cpu\n",
    "end = time.time()\n",
    "print(f\"ð§  CPU matmul time: {end - start:.6f} sec\")\n",
    "\n",
    "# GPU matmul\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "A_gpu = A_cpu.to(device)\n",
    "B_gpu = B_cpu.to(device)\n",
    "\n",
    "torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
    "\n",
    "start = time.time()\n",
    "result = A_gpu @ B_gpu\n",
    "torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
    "end = time.time()\n",
    "print(f\"â¡ GPU matmul time: {end - start:.6f} sec\")\n",
    "```\n",
    "\n",
    "### ð§ª B. Element-wise Multiply: GPU vs CPU\n",
    "\n",
    "```python\n",
    "# CPU multiply\n",
    "start = time.time()\n",
    "result = A_cpu * B_cpu\n",
    "end = time.time()\n",
    "print(f\"ð§  CPU element-wise time: {end - start:.6f} sec\")\n",
    "\n",
    "# GPU multiply\n",
    "start = time.time()\n",
    "result = A_gpu * B_gpu\n",
    "torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
    "end = time.time()\n",
    "print(f\"â¡ GPU element-wise time: {end - start:.6f} sec\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ð§  Why was your result strange?\n",
    "\n",
    "### Because:\n",
    "\n",
    "1. **we used matmul on GPU (O(nÂ³)) vs element-wise on CPU (O(n))**  \n",
    "   â Matmul is just much heavier by nature.\n",
    "\n",
    "2. **MPS backend (Apple Silicon)** is still not as optimized as CUDA for heavy matmul.  \n",
    "   â Especially true for large matrix sizes.\n",
    "\n",
    "3. **macOS MPS has no async execution detection like CUDA**  \n",
    "   â `.mps.synchronize()` helps but may not always sync perfectly in Jupyter timing.\n",
    "\n",
    "---\n",
    "\n",
    "## â What to remember:\n",
    "\n",
    "| To compare fairly | Use same operation | On both CPU & GPU |\n",
    "|-------------------|--------------------|--------------------|\n",
    "| Compare `@` vs `@`| For matrix timing   |                    |\n",
    "| Compare `*` vs `*`| For element-wise    |                    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df0a77",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "06f9f125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ð§  CPU matmul time: 0.078821 sec\n",
      "â¡ GPU matmul time: 0.051655 sec\n"
     ]
    }
   ],
   "source": [
    "# ð§ª A. Matrix Multiplication: GPU vs CPU\n",
    "import torch\n",
    "import time\n",
    "\n",
    "size = 4000  # careful! 10000 x 10000 may crash CPU RAM\n",
    "\n",
    "A_cpu = torch.rand(size, size)\n",
    "B_cpu = torch.rand(size, size)\n",
    "\n",
    "# CPU matmul\n",
    "start = time.time()\n",
    "result = A_cpu @ B_cpu\n",
    "end = time.time()\n",
    "print(f\"ð§  CPU matmul time: {end - start:.6f} sec\")\n",
    "\n",
    "# GPU matmul\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "A_gpu = A_cpu.to(device)\n",
    "B_gpu = B_cpu.to(device)\n",
    "\n",
    "torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
    "\n",
    "start = time.time()\n",
    "result = A_gpu @ B_gpu\n",
    "torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
    "end = time.time()\n",
    "print(f\"â¡ GPU matmul time: {end - start:.6f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b769d3",
   "metadata": {},
   "source": [
    "### ð The above seems to be wrong because size=4000 is `too small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c679ef5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ð§ª A. Matrix Multiplication: GPU vs CPU\n",
    "import torch\n",
    "import time\n",
    "\n",
    "size = 20000  # careful! 10000 x 10000 may crash CPU RAM\n",
    "\n",
    "A_cpu = torch.rand(size, size)\n",
    "B_cpu = torch.rand(size, size)\n",
    "\n",
    "# CPU matmul\n",
    "start = time.time()\n",
    "result = A_cpu @ B_cpu\n",
    "end = time.time()\n",
    "print(f\"ð§  CPU matmul time: {end - start:.6f} sec\")\n",
    "\n",
    "# GPU matmul\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "A_gpu = A_cpu.to(device)\n",
    "B_gpu = B_cpu.to(device)\n",
    "\n",
    "torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
    "\n",
    "start = time.time()\n",
    "result = A_gpu @ B_gpu\n",
    "torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
    "end = time.time()\n",
    "print(f\"â¡ GPU matmul time: {end - start:.6f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2ac4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ð§  CPU matmul time: 1.152387 sec\n",
      "â¡ GPU matmul time: 0.825128 sec\n"
     ]
    }
   ],
   "source": [
    "# ð§ª A. Matrix Multiplication: GPU vs CPU\n",
    "import torch\n",
    "import time\n",
    "\n",
    "size = 10000  # careful! 10000 x 10000 may crash CPU RAM\n",
    "\n",
    "A_cpu = torch.rand(size, size)\n",
    "B_cpu = torch.rand(size, size)\n",
    "\n",
    "# CPU matmul\n",
    "start = time.time()\n",
    "result = A_cpu @ B_cpu\n",
    "end = time.time()\n",
    "print(f\"ð§  CPU matmul time: {end - start:.6f} sec\")\n",
    "\n",
    "# GPU matmul\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "A_gpu = A_cpu.to(device)\n",
    "B_gpu = B_cpu.to(device)\n",
    "\n",
    "torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
    "\n",
    "start = time.time()\n",
    "result = A_gpu @ B_gpu\n",
    "torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
    "end = time.time()\n",
    "print(f\"â¡ GPU matmul time: {end - start:.6f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d8466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ð§  CPU element-wise time: 0.040624 sec\n",
      "â¡ GPU element-wise time: 0.062440 sec\n"
     ]
    }
   ],
   "source": [
    "# ð§ª B. Element-wise Multiply: GPU vs CPU\n",
    "\n",
    "# CPU multiply\n",
    "start = time.time()\n",
    "result = A_cpu * B_cpu\n",
    "end = time.time()\n",
    "print(f\"ð§  CPU element-wise time: {end - start:.6f} sec\")\n",
    "\n",
    "# GPU multiply\n",
    "start = time.time()\n",
    "result = A_gpu * B_gpu\n",
    "torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
    "end = time.time()\n",
    "print(f\"â¡ GPU element-wise time: {end - start:.6f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101f39aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ð§  CPU matmul time: 1.117793 sec\n",
      "â¡ GPU matmul time: 0.832834 sec\n",
      "ð§  CPU element-wise time: 0.030282 sec\n",
      "â¡ GPU element-wise time: 0.008068 sec\n"
     ]
    }
   ],
   "source": [
    "# ð§ª A. Matrix Multiplication: GPU vs CPU\n",
    "import torch\n",
    "import time\n",
    "\n",
    "size = 10000  # careful! 10000 x 10000 may crash CPU RAM\n",
    "\n",
    "A_cpu = torch.rand(size, size)\n",
    "B_cpu = torch.rand(size, size)\n",
    "\n",
    "# CPU matmul\n",
    "start = time.time()\n",
    "result = A_cpu @ B_cpu\n",
    "end = time.time()\n",
    "print(f\"ð§  CPU matmul time: {end - start:.6f} sec\")\n",
    "\n",
    "# GPU matmul\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "A_gpu = A_cpu.to(device)\n",
    "B_gpu = B_cpu.to(device)\n",
    "\n",
    "torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
    "\n",
    "start = time.time()\n",
    "result = A_gpu @ B_gpu\n",
    "torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
    "end = time.time()\n",
    "print(f\"â¡ GPU matmul time: {end - start:.6f} sec\")\n",
    "\n",
    "\n",
    "# ð§ª B. Element-wise Multiply: GPU vs CPU\n",
    "\n",
    "# CPU multiply\n",
    "start = time.time()\n",
    "result = A_cpu * B_cpu\n",
    "end = time.time()\n",
    "print(f\"ð§  CPU element-wise time: {end - start:.6f} sec\")\n",
    "\n",
    "# GPU multiply\n",
    "start = time.time()\n",
    "result = A_gpu * B_gpu\n",
    "torch.cuda.synchronize() if device == \"cuda\" else torch.mps.synchronize()\n",
    "end = time.time()\n",
    "print(f\"â¡ GPU element-wise time: {end - start:.6f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79e77c",
   "metadata": {},
   "source": [
    "### ð Now this makes lots of sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6d318d",
   "metadata": {},
   "source": [
    "---\n",
    "### Setting Time\n",
    "\n",
    "#### ð Summary of Timing Methods\n",
    "\n",
    "| Method       | Scope           | Use Case                        | Output                      |\n",
    "|--------------|------------------|----------------------------------|-----------------------------|\n",
    "| `time.time()`| Python (standard)| Custom timing for code blocks   | Seconds (float)             |\n",
    "| `%%time`     | Jupyter magic    | Time the **entire cell once**   | Wall time & CPU time shown |\n",
    "| `%%timeit`   | Jupyter magic    | Time the cell **multiple times**, auto-averaged | Best of N runs (stats)     |\n",
    "\n",
    "---\n",
    "\n",
    "##### ð `%%time` vs `%%timeit`\n",
    "\n",
    "| Feature        | `%%time`                | `%%timeit`                    |\n",
    "|----------------|-------------------------|-------------------------------|\n",
    "| Runs           | 1                       | Many (auto-determined)        |\n",
    "| Output         | One-time duration       | Average + std dev             |\n",
    "| Overhead       | None                    | Slight (due to multiple runs) |\n",
    "| Use for        | Rough profiling         | Micro-benchmarking            |\n",
    "\n",
    "---\n",
    "\n",
    "#### ð§° Timer Decorator (for any function)\n",
    "\n",
    "You can use this to time any Python function:\n",
    "\n",
    "```python\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def timer(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"â±ï¸ Function '{func.__name__}' executed in {end - start:.6f} sec\")\n",
    "        return result\n",
    "    return wrapper\n",
    "```\n",
    "\n",
    "##### ð§ª Example usage:\n",
    "\n",
    "```python\n",
    "@timer\n",
    "def slow_function():\n",
    "    total = 0\n",
    "    for i in range(10**7):\n",
    "        total += i\n",
    "    return total\n",
    "\n",
    "slow_function()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c7ab7",
   "metadata": {},
   "source": [
    "#### 1. `@timer_time` based on `time.time()` (custom Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c53121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer_time] 'loop_sum' executed in 0.03027511 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "499999500000"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def timer_time(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        # print(f\"elapsed_time = {end - start:.8f}\")\n",
    "        print(f\"[timer_time] '{func.__name__}' executed in {end - start:.8f} sec\")\n",
    "        return result\n",
    "    return wrapper\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "@timer_time\n",
    "def loop_sum():\n",
    "    return sum(i for i in range(10**6))\n",
    "    \n",
    "loop_sum()  # In Jupyter, the final stmt in a cell will be printed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f7612d",
   "metadata": {},
   "source": [
    "#### If we `don't want to return a value at the end stmt` of Jupyter cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed52b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer_time] 'loop_sum' executed in 0.02570796 sec\n"
     ]
    }
   ],
   "source": [
    "_= loop_sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492bf8da",
   "metadata": {},
   "source": [
    "#### 2. `@timer_cell` -- simulating `%%time` for full function  (wall time + CPU time)\n",
    "\n",
    "> - `wall time` = `time.time()` and is the clock (on the wall) time, which is the time you feel, the total time spent\n",
    "> - `ru_utime` (retrieved from `resources`'s `SELF` for this specific program), is the time that this program acutually executed the CPU instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514ae7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall time = 0.03859687\n",
      "cpu_time  = 0.03379300\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import resource\n",
    "from functools import wraps\n",
    "\n",
    "def timer_cell(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_wall = time.time()\n",
    "        start_cpu = resource.getrusage(resource.RUSAGE_SELF).ru_utime\n",
    "        # start_res = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        end_wall = time.time()\n",
    "        end_cpu = resource.getrusage(resource.RUSAGE_SELF).ru_utime\n",
    "        # end_res = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "        \n",
    "        wall_time = end_wall - start_wall\n",
    "        cpu_time = end_cpu - start_cpu\n",
    "        \n",
    "        # elapsed_cpu = end_cpu - start_cpu\n",
    "        # elapsed_res = end_res - start_res\n",
    "        \n",
    "        print(f\"wall time = {wall_time:.8f}\")\n",
    "        print(f\"cpu_time  = {cpu_time:.8f}\")\n",
    "        return result\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "# Example usage\n",
    "@timer_cell\n",
    "def list_comprehension():\n",
    "    return [i**2 for i in range(10**6)]\n",
    "\n",
    "_ = list_comprehension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9a8f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer_cell] 'list_comprehension' wall time: 0.043481 sec\n",
      "[timer_cell] 'list_comprehension' CPU time:  0.034716 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import resource\n",
    "from functools import wraps\n",
    "\n",
    "def timer_cell(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_wall = time.time()\n",
    "        start_cpu = resource.getrusage(resource.RUSAGE_SELF).ru_utime\n",
    "        result = func(*args, **kwargs)\n",
    "        end_wall = time.time()\n",
    "        end_cpu = resource.getrusage(resource.RUSAGE_SELF).ru_utime\n",
    "        print(f\"[timer_cell] '{func.__name__}' wall time: {end_wall - start_wall:.6f} sec\")\n",
    "        print(f\"[timer_cell] '{func.__name__}' CPU time:  {end_cpu - start_cpu:.6f} sec\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Example usage\n",
    "@timer_cell\n",
    "def list_comprehension():\n",
    "    return [i**2 for i in range(10**6)]\n",
    "\n",
    "_ = list_comprehension()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc3e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499999500000"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some other example for sleep(2) to make difference of wall/cpu time\n",
    "\n",
    "import time\n",
    "\n",
    "def sleepy():\n",
    "    time.sleep(2)\n",
    "    return sum(range(10**6))\n",
    "\n",
    "sleepy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100836f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer_cell] 'sleepy' wall time: 2.017744 sec\n",
      "[timer_cell] 'sleepy' CPU time:  0.013933 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "499999500000"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "@timer_cell\n",
    "def sleepy():\n",
    "    time.sleep(2)\n",
    "    return sum(range(10**6))\n",
    "\n",
    "sleepy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0081a0",
   "metadata": {},
   "source": [
    "#### 3. `@timer_timeit` -- Mimicking `%%timeit` using `timeit.repeat()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fa340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer_timeit] 'multiply_loop' best of 5 Ã 3 runs: 0.000155 sec/run\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit\n",
    "from functools import wraps\n",
    "\n",
    "def timer_timeit(n=5, r=3):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            stmt = lambda: func(*args, **kwargs)\n",
    "            times = timeit.repeat(stmt, repeat=r, number=n)\n",
    "            best_time = min(times) / n\n",
    "            print(f\"[timer_timeit] '{func.__name__}' best of {r} Ã {n} runs: {best_time:.6f} sec/run\")\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return  decorator\n",
    "\n",
    "# Example usage\n",
    "@timer_timeit(n=3, r=5)\n",
    "def multiply_loop():\n",
    "    x = 1\n",
    "    for i in range(1, 10000):\n",
    "        x *= 1\n",
    "    return x\n",
    "\n",
    "multiply_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a211c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer_timeit] 'multiply_loop' best    of 30 Ã 20 runs: 0.000146 sec/run\n",
      "[timer_timeit] 'multiply_loop' worst   of 30 Ã 20 runs: 0.000310 sec/run\n",
      "[timer_timeit] 'multiply_loop' average of 30 Ã 20 runs: 0.000177 sec/run\n",
      "[timer_timeit] 'multiply_loop' median  of 30 Ã 20 runs: 0.000153 sec/run\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit\n",
    "from functools import wraps\n",
    "import statistics  ## for calc mean and median times\n",
    "\n",
    "def timer_timeit(n=5, r=3):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            stmt = lambda: func(*args, **kwargs)\n",
    "            times = timeit.repeat(stmt, repeat=r, number=n)\n",
    "            \n",
    "            best_time = min(times) / n\n",
    "            worst_time = max(times) / n\n",
    "            avg_time = statistics.mean(times) / n\n",
    "            median_time = statistics.median(times) / n\n",
    "            \n",
    "            print(f\"[timer_timeit] '{func.__name__}' best    of {r} Ã {n} runs: {best_time:.6f} sec/run\")\n",
    "            print(f\"[timer_timeit] '{func.__name__}' worst   of {r} Ã {n} runs: {worst_time:.6f} sec/run\")\n",
    "            print(f\"[timer_timeit] '{func.__name__}' average of {r} Ã {n} runs: {avg_time:.6f} sec/run\")\n",
    "            print(f\"[timer_timeit] '{func.__name__}' median  of {r} Ã {n} runs: {median_time:.6f} sec/run\")\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return  decorator\n",
    "\n",
    "# Example usage\n",
    "@timer_timeit(n=20, r=30)\n",
    "def multiply_loop():\n",
    "    x = 1\n",
    "    for i in range(1, 10000):\n",
    "        x *= 1\n",
    "    return x\n",
    "\n",
    "multiply_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24006b7",
   "metadata": {},
   "source": [
    "---\n",
    "## %%time and some torch operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b46f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5db1deb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse = 0.001039\n",
      "CPU times: user 508 Î¼s, sys: 1.27 ms, total: 1.78 ms\n",
      "Wall time: 1.17 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = time.time()\n",
    "\n",
    "# matrix operation here\n",
    "zeros = torch.zeros(1, 1)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "elapse = end - start\n",
    "print(f\"elapse = {elapse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d434b",
   "metadata": {},
   "source": [
    "### PyTorch Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ce86e",
   "metadata": {},
   "source": [
    "| Function                      | Main Purpose                          |\n",
    "|------------------------------|----------------------------------------|\n",
    "| `torch.stack`                | Stack tensors along a **new dimension** |\n",
    "| `torch.cat`                  | Concatenate tensors along an **existing dimension** |\n",
    "| `torch.multinomial`          | Sample based on given probabilities     |\n",
    "| `torch.tril` / `torch.triu`  | Extract lower / upper triangular matrix |\n",
    "| `input.T` / `transpose`      | Transpose a tensor (2D or more needs2use `transpose`)         |\n",
    "| `nn.Linear`                  | Create a fully connected (linear) layer |\n",
    "| `F.softmax`                  | Convert logits to probabilities         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf5c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.stack, torch.multinomial, torch.trill, torch.trilu, input.T / input.transpose, nn.Linear, torch.cat, F.softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc3d04",
   "metadata": {},
   "source": [
    "#### `torch.stack(tensors, dim=0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b6e39f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.stack(tensors, dim=0)\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2])\n",
    "b = torch.tensor([3, 4])\n",
    "\n",
    "stacked = torch.stack([a, b],  dim=0)\n",
    "\n",
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "44090ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3],\n",
       "        [2, 4]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a, b], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5367a4c1",
   "metadata": {},
   "source": [
    "#### `torch.cat` - use this to generating text giving the context\n",
    "- adding the predicted word and concatenate to the current chunk of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f8257682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1, 2, 3, 4,])\n",
    "out = torch.cat((tensor, torch.tensor([5, 6])), 0)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda89e7a",
   "metadata": {},
   "source": [
    "####  `torch.multinomial(input, num_samples, replacement=False)`\n",
    "`we will use this to predict what word will come next æåç¨éä¾é æ¸¬ä¸ä¸åå­`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7e0f32d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# Torch.multinomial(input, num_samples, replacement=False)\n",
    "# Sampling based on given probability (æ ¹æçµ¦å®çæ©çåä½é²è¡æ½æ¨£)\n",
    "\n",
    "# [index]ï¼probability -> [0]:0.1, [1]:0.3, [2]:0.6\n",
    "weights = torch.tensor([0.1, 0.3, 0.6])  # this is the given prob\n",
    "\n",
    "samples = torch.multinomial(weights, 5, replacement=True)\n",
    "print(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "49aefb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of index 0: 5, dist P: 0.05\n",
      "Count of index 1: 41, dist P: 0.41\n",
      "Count of index 2: 54, dist P: 0.54\n"
     ]
    }
   ],
   "source": [
    "weights = torch.tensor([0.1, 0.3, 0.6])\n",
    "\n",
    "samples = torch.multinomial(weights, 100, replacement=True)\n",
    "\n",
    "# use bincount to calc the freq of each digits\n",
    "counts = torch.bincount(samples)\n",
    "print(f\"Count of index 0: {counts[0]}, dist P: {counts[0] / 100:.2f}\")\n",
    "print(f\"Count of index 1: {counts[1]}, dist P: {counts[1] / 100:.2f}\")\n",
    "print(f\"Count of index 2: {counts[2]}, dist P: {counts[2] / 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76788616",
   "metadata": {},
   "source": [
    "#### 3. `torch.trill(input, diagonal=0)` and `triu(input, diagonal=0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "29d831cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  0,  0,  0,  0,  0],\n",
      "        [ 7,  8,  0,  0,  0,  0],\n",
      "        [13, 14, 15,  0,  0,  0],\n",
      "        [19, 20, 21, 22,  0,  0],\n",
      "        [25, 26, 27, 28, 29,  0],\n",
      "        [31, 32, 33, 34, 35, 36]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 37).reshape(6, 6)\n",
    "print(torch.tril(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "64de263a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6],\n",
       "        [ 0,  8,  9, 10, 11, 12],\n",
       "        [ 0,  0, 15, 16, 17, 18],\n",
       "        [ 0,  0,  0, 22, 23, 24],\n",
       "        [ 0,  0,  0,  0, 29, 30],\n",
       "        [ 0,  0,  0,  0,  0, 36]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1, 37).reshape(6, 6)\n",
    "torch.triu(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa21bfc",
   "metadata": {},
   "source": [
    "#### `input.T` vs `input.transpose(dim0, dim1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b3a698fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9166,  0.4310, -0.5345],\n",
       "        [-0.1544,  0.1354, -0.7043]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f238d542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3581, -0.2482],\n",
      "        [-0.5619, -0.3787],\n",
      "        [-0.8063,  0.5485]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf44ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "x3d = torch.randn(2, 3, 4)\n",
    "print(x3d.transpose(1, 2).shape) # 1 means index 1 => 3, 2 means index 2 => 4\n",
    "\n",
    "# therefore, x3d = (2, 3, 4) -> (2, 4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8c595",
   "metadata": {},
   "source": [
    "#### `nn.Linear(in_features, out_features)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6be367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "layer = nn.Linear(4, 2)    # y = xW^T + b => in: 4, out: 2\n",
    "input = torch.randn(3, 4)  # (batch_size :3 , in_features: 4)\n",
    "output = layer(input)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "170d2eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "01e0272b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(4, 2)  # in_features: 4, out_features: 2\n",
    "x = torch.rand(32, 4)  # 32 samples (batch size=32), each w/4 features\n",
    "output = layer(x)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d9e4f",
   "metadata": {},
   "source": [
    "| concept            | description                                       |\n",
    "|------------------|--------------------------------------------|\n",
    "| `nn.Linear(m, n)`| convert each data item from m dim to n dim    |\n",
    "| `x.shape`        | must be `[batch_size, in_features]`          |\n",
    "| `output.shape`   | result will be `[batch_size, out_features]`       |\n",
    "| `batch size`     | concurrent process multi-data, faster, stable gradient     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de56b3a",
   "metadata": {},
   "source": [
    "#### 7. `F.softmax` : convert `logits` into `softmax` probs. (sum up to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c18a4c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0321, 0.0871, 0.2369, 0.6439])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "probs = F.softmax(x, dim=0)\n",
    "\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74efce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "llm-scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
