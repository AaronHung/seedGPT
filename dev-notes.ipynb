{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d73bdb6",
   "metadata": {},
   "source": [
    "## ü™¥ seedGPT\n",
    "\n",
    "**A seed-sized, Pythonic GPT-2 implementation from scratch.**  \n",
    "No frameworks. No frills. Just pure attention.\n",
    "\n",
    "> _\"Every forest begins with a seed.\"_  \n",
    "> `seedGPT` is a minimal yet faithful implementation of the GPT-2 architecture, handcrafted from the ground up using vanilla Python and PyTorch.  \n",
    "> It's designed as a **starting point** ‚Äî to learn, experiment, and grow your own language model ecosystem.\n",
    "\n",
    "---\n",
    "\n",
    "### üå± Highlights\n",
    "\n",
    "- GPT-2 style Transformer decoder\n",
    "- Clean training and inference loops\n",
    "- From scratch, no external GPT libraries\n",
    "- Ideal for beginners, researchers, and tinkerers\n",
    "\n",
    "---\n",
    "\n",
    "- Aaron Hung\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b5f9db",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ab452",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### üìö **Corpus Download in One Line**\n",
    "\n",
    "We use **Project Gutenberg** to download large-scale, high-quality literary texts for language model training.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **Creative Alternatives to Gutenberg**\n",
    "\n",
    "1. **Wikipedia Dumps** (via `wikiextractor` or Hugging Face):\n",
    "   - High coverage, factual, multi-language.\n",
    "   - Great for general knowledge and clean sentence structure.\n",
    "\n",
    "2. **Common Crawl**:\n",
    "   - Massive web-scale data, noisy but diverse.\n",
    "   - Used by many large models (e.g., GPT-3, LLaMA).\n",
    "\n",
    "3. **OpenSubtitles**:\n",
    "   - Dialog-style corpora from movies and TV.\n",
    "   - Great for conversational tone and multilingual content (e.g., Swedish, Taiwanese, Moroccan Arabic).\n",
    "\n",
    "4. **Books3** (from ThePile):\n",
    "   - Rich English literature, more modern and diverse than Gutenberg.\n",
    "   - Contains fiction, non-fiction, some controversy over copyright.\n",
    "\n",
    "5. **Tatoeba Project**:\n",
    "   - Sentence-aligned multilingual database.\n",
    "   - Especially useful for under-resourced languages like **Minnan/Taiwanese**, **Moroccan Darija**, **Swedish**, etc.\n",
    "\n",
    "6. **CC100**:\n",
    "   - A cleaned version of Common Crawl available in over 100 languages.\n",
    "   - Good for training multilingual models.\n",
    "\n",
    "7. **Language-specific Wikis & Blogs**:\n",
    "   - For example, **Êë©Ê¥õÂì• Darija (Arabic dialect)** ‚Üí scrape local forums, YouTube comments.\n",
    "   - **ÁπÅÈ´î‰∏≠Êñá** ‚Üí zh.wikisource.org, Chinese literature sites like open-lit.com.\n",
    "   - **Âè∞Ë™ûÔºàMinnanÔºâ** ‚Üí Taiwanese bible corpora, folk song lyrics, gov.tw resources.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **What Did Karpathy Use for nanoGPT?**\n",
    "\n",
    "Karpathy trained `nanoGPT` on:\n",
    "- **TinyShakespeare** (sample)  \n",
    "- **OpenWebText** (for full-scale runs, similar to GPT-2)  \n",
    "- **The Pile** (optional, contains Books3, arXiv, etc.)\n",
    "\n",
    "**Advantages:**\n",
    "- Highly replicable and clean structure\n",
    "- Small enough to understand learning dynamics\n",
    "- Great for bootstrapping/debugging models\n",
    "\n",
    "**Disadvantages:**\n",
    "- Not diverse\n",
    "- Lacks multilinguality\n",
    "- Not suitable for production-grade models\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Comparison Table**\n",
    "\n",
    "| Source              | Size     | Quality  | Diversity     | Languages     | License Issues |\n",
    "|---------------------|----------|----------|---------------|---------------|----------------|\n",
    "| Project Gutenberg   | Medium   | High     | Mostly English classics | English, some others | ‚úÖ Free |\n",
    "| Karpathy Datasets   | Small    | Very High| Low (focused) | English only  | ‚úÖ Free |\n",
    "| Common Crawl        | Huge     | Mixed    | Very High     | üåç 100+        | ‚ö†Ô∏è Mixed |\n",
    "| OpenSubtitles       | Large    | Medium   | Dialogue-heavy| Many          | ‚ö†Ô∏è Grey area |\n",
    "| Tatoeba             | Small    | High     | Sentence-level| üí¨ 400+        | ‚úÖ Free |\n",
    "| CC100               | Huge     | Medium   | Web-style     | üåê 100+        | ‚úÖ Free |\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ **Recommendation for Special Languages**\n",
    "\n",
    "- **ÁπÅÈ´î‰∏≠Êñá (Traditional Chinese)**: Use zh.wikisource, Chinese Open-Lit, or scrape Taiwanese news sites.\n",
    "- **Âè∞Ë™û (Minnan)**: Bible corpora, gov.tw local dialect projects, or oral transcription data.\n",
    "- **Moroccan Darija**: Hard to find! Try scraping Facebook/YT comments, local forums, WhatsApp corpora.\n",
    "- **Swedish**: Use `sv.wikipedia`, OpenSubtitles in Swedish, and local government documents (open data portals).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3710502e",
   "metadata": {},
   "source": [
    "### For now, the Corpus will be:\n",
    "from `Project Gutenburg` -- `Dorothy and the Wizard in Oz`\n",
    "https://www.gutenberg.org/ebooks/22566\n",
    "in Plain-Text UTF8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa0d4d4",
   "metadata": {},
   "source": [
    "### üêô Some Dev-Notes:\n",
    "- install pylzma for unzipping files for Windows\n",
    "- but pylzma might not work for your MAC/Linux, consider to use `py7zr`, or standard library `lzma`, or `zipfile`, `tarfile` built-in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c7dbd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ludwig-llm",
   "language": "python",
   "name": "ludwig-llm"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
